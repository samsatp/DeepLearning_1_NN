{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. A basic Value wrapper for a scalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Value:\n",
    "    def __init__(self, data: float, label: str = \"\") -> None:\n",
    "        self.data =  data\n",
    "        self.label = label\n",
    "        self.grad = 0.0\n",
    "        self._backward = lambda : None\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        if self.label != \"\":\n",
    "            return f\"Value(label = {self.label}, data = {self.data})\"\n",
    "        return f\"Value(data = {self.data})\"\n",
    "\n",
    "    def __add__(self, other):\n",
    "        return Value(data = self.data + other.data)\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        return Value(data = self.data * other.data)\n",
    "\n",
    "    def __rmul__(self, other):\n",
    "        return self * other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Value(data = 3.0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = Value(data=1.0, label='a')\n",
    "b = Value(data=2.0, label='b')\n",
    "\n",
    "a + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Value(data = 2.0)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a * b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Value(data = 2.0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b * a"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Define backward function\n",
    "For each operation, we are creating a new Value object derived by the callee operation. That object must embed information for how to propagate gradient backward through it. This gradient is calculated by Chain rule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Value:\n",
    "    def __init__(self, data: float, label: str = \"\") -> None:\n",
    "        self.data =  data\n",
    "        self.label = label\n",
    "        self.grad = 0.0\n",
    "        self._backward = lambda : None\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        if self.label != \"\":\n",
    "            return f\"Value(label = {self.label}, data = {self.data})\"\n",
    "        return f\"Value(data = {self.data})\"\n",
    "\n",
    "    def __add__(self, other):\n",
    "        # Forward pass\n",
    "        other = Value(data=other) if not isinstance(other, Value) else other\n",
    "        output = Value(data = self.data + other.data)\n",
    "\n",
    "        # Backward pass function\n",
    "        def _backward():\n",
    "            self.grad += output.grad * 1.0   # We use += because gradients can be accumulated from many paths\n",
    "            other.grad += output.grad * 1.0  # depending on how many nodes this self node connects to.\n",
    "        output._backward = _backward\n",
    "        \n",
    "        return output\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        # Forward pass\n",
    "        output = Value(data = self.data * other.data)\n",
    "\n",
    "        # Backward pass function\n",
    "        def _backward():\n",
    "            self.grad += output.grad * other.data\n",
    "            other.grad += output.grad * self.data\n",
    "        output._backward = _backward\n",
    "\n",
    "        return output\n",
    "\n",
    "    def tanh(self):\n",
    "        # Forward pass\n",
    "        x = self.data\n",
    "        t = (math.exp(2*x) - 1)/(math.exp(2*x) + 1)\n",
    "        output = Value(data = t)\n",
    "        \n",
    "        # Backward pass function\n",
    "        def _backward():\n",
    "            self.grad += (1 - t**2) * output.grad\n",
    "        output._backward = _backward\n",
    "\n",
    "        return output\n",
    "        \n",
    "    def __rmul__(self, other):\n",
    "        return self * other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs x1,x2\n",
    "x1 = Value(2.0, label='x1')\n",
    "x2 = Value(0.0, label='x2')\n",
    "# weights w1,w2\n",
    "w1 = Value(-3.0, label='w1')\n",
    "w2 = Value(1.0, label='w2')\n",
    "# bias of the neuron\n",
    "b = Value(6.8813735870195432, label='b')\n",
    "# x1*w1 + x2*w2 + b\n",
    "x1w1 = x1*w1; x1w1.label = 'x1*w1'\n",
    "x2w2 = x2*w2; x2w2.label = 'x2*w2'\n",
    "x1w1x2w2 = x1w1 + x2w2; x1w1x2w2.label = 'x1*w1 + x2*w2'\n",
    "n = x1w1x2w2 + b; n.label = 'n'\n",
    "o = n.tanh(); o.label = 'o'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data:\n",
      "    o: 0.707\n",
      "    n: 0.881\n",
      "    b: 6.881\n",
      "    x1w1x2w2: -6.000\n",
      "    x1w1: -6.000\n",
      "    x2w2: 0.000\n",
      "    x1: 2.000\n",
      "    w1: -3.000\n",
      "    x2: 0.000\n",
      "    w2: 1.000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"\"\"\n",
    "Data:\n",
    "    o: {o.data:.3f}\n",
    "    n: {n.data:.3f}\n",
    "    b: {b.data:.3f}\n",
    "    x1w1x2w2: {x1w1x2w2.data:.3f}\n",
    "    x1w1: {x1w1.data:.3f}\n",
    "    x2w2: {x2w2.data:.3f}\n",
    "    x1: {x1.data:.3f}\n",
    "    w1: {w1.data:.3f}\n",
    "    x2: {x2.data:.3f}\n",
    "    w2: {w2.data:.3f}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Gradient:\n",
      "    o: 1.000\n",
      "    n: 0.500\n",
      "    b: 0.500\n",
      "    x1w1x2w2: 0.500\n",
      "    x1w1: 0.500\n",
      "    x2w2: 0.500\n",
      "    x1: -1.500\n",
      "    w1: 1.000\n",
      "    x2: 0.500\n",
      "    w2: 0.000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    To start up gradient backpropagation, the final node's gradient is set to one.\n",
    "    e.g. the gradient of the loss function wrt. itself is one.\n",
    "\"\"\"\n",
    "\n",
    "o.grad = 1.0\n",
    "\n",
    "\"\"\"\n",
    "    We need call backward function for parent before children for every pair of them\n",
    "    so that gradient flows backward in a correct order.\n",
    "\"\"\"\n",
    "o._backward()\n",
    "n._backward()\n",
    "b._backward()\n",
    "x1w1x2w2._backward()\n",
    "x1w1._backward()\n",
    "x2w2._backward()\n",
    "\n",
    "print(f\"\"\"\n",
    "Gradient:\n",
    "    o: {o.grad:.3f}\n",
    "    n: {n.grad:.3f}\n",
    "    b: {b.grad:.3f}\n",
    "    x1w1x2w2: {x1w1x2w2.grad:.3f}\n",
    "    x1w1: {x1w1.grad:.3f}\n",
    "    x2w2: {x2w2.grad:.3f}\n",
    "    x1: {x1.grad:.3f}\n",
    "    w1: {w1.grad:.3f}\n",
    "    x2: {x2.grad:.3f}\n",
    "    w2: {w2.grad:.3f}\n",
    "\"\"\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Convenient backward function\n",
    "Calling backward funtion for each node manually is tedious. We need a function that triggers backward propagation from the end back to the intput nodes automatically.\n",
    "\n",
    "To do so, we need a way to traverse through the computational graph in the right order. So, we need to keep additional information as to how each node is constructed (e.g. its previous nodes)\n",
    "\n",
    "c = a + b <br>\n",
    "c.previous = (a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Value:\n",
    "    def __init__(self, data: float, label: str = \"\", previous = []) -> None:\n",
    "        self.data =  data\n",
    "        self.label = label\n",
    "        self.grad = 0.0\n",
    "        self._backward = lambda : None\n",
    "        self._previous = previous\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        if self.label != \"\":\n",
    "            return f\"Value(label = {self.label}, data = {self.data})\"\n",
    "        return f\"Value(data = {self.data})\"\n",
    "\n",
    "    def __add__(self, other):\n",
    "        # Forward pass\n",
    "        other = Value(data=other) if not isinstance(other, Value) else other\n",
    "        output = Value(data = self.data + other.data, previous=[self, other])\n",
    "\n",
    "        # Backward pass function\n",
    "        def _backward():\n",
    "            self.grad += output.grad * 1.0\n",
    "            other.grad += output.grad * 1.0\n",
    "        output._backward = _backward\n",
    "        \n",
    "        return output\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        # Forward pass\n",
    "        other = Value(data=other) if not isinstance(other, Value) else other\n",
    "        output = Value(data = self.data * other.data, previous=[self, other])\n",
    "\n",
    "        # Backward pass function\n",
    "        def _backward():\n",
    "            self.grad += output.grad * other.data\n",
    "            other.grad += output.grad * self.data\n",
    "        output._backward = _backward\n",
    "\n",
    "        return output\n",
    "\n",
    "    def tanh(self):\n",
    "        # Forward pass\n",
    "        x = self.data\n",
    "        t = (math.exp(2*x) - 1)/(math.exp(2*x) + 1)\n",
    "        output = Value(data = t, previous=[self])\n",
    "        \n",
    "        # Backward pass function\n",
    "        def _backward():\n",
    "            self.grad += (1 - t**2) * output.grad\n",
    "        output._backward = _backward\n",
    "\n",
    "        return output\n",
    "        \n",
    "    def __rmul__(self, other):\n",
    "        return self * other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs x1,x2\n",
    "x1 = Value(2.0, label='x1')\n",
    "x2 = Value(0.0, label='x2')\n",
    "# weights w1,w2\n",
    "w1 = Value(-3.0, label='w1')\n",
    "w2 = Value(1.0, label='w2')\n",
    "# bias of the neuron\n",
    "b = Value(6.8813735870195432, label='b')\n",
    "# x1*w1 + x2*w2 + b\n",
    "x1w1 = x1*w1; x1w1.label = 'x1*w1'\n",
    "x2w2 = x2*w2; x2w2.label = 'x2*w2'\n",
    "x1w1x2w2 = x1w1 + x2w2; x1w1x2w2.label = 'x1*w1 + x2*w2'\n",
    "n = x1w1x2w2 + b; n.label = 'n'\n",
    "o = n.tanh(); o.label = 'o'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use Breadth First Search to find the correct topological order of gradient backward propagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bfs(root): #function for BFS\n",
    "    visited = []\n",
    "    queue = []\n",
    "    visited.append(root)\n",
    "    queue.append(root)\n",
    "\n",
    "    out = []\n",
    "    while queue:          # Creating loop to visit each node\n",
    "        m = queue.pop(0) \n",
    "        out.append(m)\n",
    "\n",
    "        for neighbour in m._previous:\n",
    "            if neighbour not in visited:\n",
    "                visited.append(neighbour)\n",
    "                queue.append(neighbour)\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Value(label = o, data = 0.7071067811865476),\n",
       " Value(label = n, data = 0.8813735870195432),\n",
       " Value(label = x1*w1 + x2*w2, data = -6.0),\n",
       " Value(label = b, data = 6.881373587019543),\n",
       " Value(label = x1*w1, data = -6.0),\n",
       " Value(label = x2*w2, data = 0.0),\n",
       " Value(label = x1, data = 2.0),\n",
       " Value(label = w1, data = -3.0),\n",
       " Value(label = x2, data = 0.0),\n",
       " Value(label = w2, data = 1.0)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bfs(o)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, when we propagate gradient by this order, the gradient will flow correctly from output to input nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Value:\n",
    "    def __init__(self, data: float, label: str = \"\", previous = []) -> None:\n",
    "        self.data =  data\n",
    "        self.label = label\n",
    "        self.grad = 0.0\n",
    "        self._backward = lambda : None\n",
    "        self._previous = previous\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        if self.label != \"\":\n",
    "            return f\"Value(label = {self.label}, data = {self.data})\"\n",
    "        return f\"Value(data = {self.data})\"\n",
    "\n",
    "    def __add__(self, other):\n",
    "        # Forward pass\n",
    "        other = Value(data=other) if not isinstance(other, Value) else other\n",
    "        output = Value(data = self.data + other.data, previous=[self, other])\n",
    "\n",
    "        # Backward pass function\n",
    "        def _backward():\n",
    "            self.grad += output.grad * 1.0\n",
    "            other.grad += output.grad * 1.0\n",
    "        output._backward = _backward\n",
    "        \n",
    "        return output\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        # Forward pass\n",
    "        other = Value(data=other) if not isinstance(other, Value) else other\n",
    "        output = Value(data = self.data * other.data, previous=[self, other])\n",
    "\n",
    "        # Backward pass function\n",
    "        def _backward():\n",
    "            self.grad += output.grad * other.data\n",
    "            other.grad += output.grad * self.data\n",
    "        output._backward = _backward\n",
    "\n",
    "        return output\n",
    "\n",
    "    def tanh(self):\n",
    "        # Forward pass\n",
    "        x = self.data\n",
    "        t = (math.exp(2*x) - 1)/(math.exp(2*x) + 1)\n",
    "        output = Value(data = t, previous=[self])\n",
    "        \n",
    "        # Backward pass function\n",
    "        def _backward():\n",
    "            self.grad += (1 - t**2) * output.grad\n",
    "        output._backward = _backward\n",
    "\n",
    "        return output\n",
    "        \n",
    "    def __rmul__(self, other):\n",
    "        return self * other\n",
    "\n",
    "    def backward(self):\n",
    "        self.grad = 1.0\n",
    "        def bfs(root): #function for BFS\n",
    "            visited = []\n",
    "            queue = []\n",
    "            visited.append(root)\n",
    "            queue.append(root)\n",
    "\n",
    "            out = []\n",
    "            while queue:          # Creating loop to visit each node\n",
    "                m = queue.pop(0) \n",
    "                out.append(m)\n",
    "\n",
    "                for neighbour in m._previous:\n",
    "                    if neighbour not in visited:\n",
    "                        visited.append(neighbour)\n",
    "                        queue.append(neighbour)\n",
    "\n",
    "            return out\n",
    "\n",
    "        output_order = bfs(self)\n",
    "        for node in output_order:\n",
    "            node._backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Gradient:\n",
      "    o: 1.000\n",
      "    n: 0.500\n",
      "    b: 0.500\n",
      "    x1w1x2w2: 0.500\n",
      "    x1w1: 0.500\n",
      "    x2w2: 0.500\n",
      "    x1: -1.500\n",
      "    w1: 1.000\n",
      "    x2: 0.500\n",
      "    w2: 0.000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# inputs x1,x2\n",
    "x1 = Value(2.0, label='x1')\n",
    "x2 = Value(0.0, label='x2')\n",
    "# weights w1,w2\n",
    "w1 = Value(-3.0, label='w1')\n",
    "w2 = Value(1.0, label='w2')\n",
    "# bias of the neuron\n",
    "b = Value(6.8813735870195432, label='b')\n",
    "# x1*w1 + x2*w2 + b\n",
    "x1w1 = x1*w1; x1w1.label = 'x1*w1'\n",
    "x2w2 = x2*w2; x2w2.label = 'x2*w2'\n",
    "x1w1x2w2 = x1w1 + x2w2; x1w1x2w2.label = 'x1*w1 + x2*w2'\n",
    "n = x1w1x2w2 + b; n.label = 'n'\n",
    "o = n.tanh(); o.label = 'o'\n",
    "\n",
    "o.backward()\n",
    "\n",
    "print(f\"\"\"\n",
    "Gradient:\n",
    "    o: {o.grad:.3f}\n",
    "    n: {n.grad:.3f}\n",
    "    b: {b.grad:.3f}\n",
    "    x1w1x2w2: {x1w1x2w2.grad:.3f}\n",
    "    x1w1: {x1w1.grad:.3f}\n",
    "    x2w2: {x2w2.grad:.3f}\n",
    "    x1: {x1.grad:.3f}\n",
    "    w1: {w1.grad:.3f}\n",
    "    x2: {x2.grad:.3f}\n",
    "    w2: {w2.grad:.3f}\n",
    "\"\"\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = Value(-2.0, label='a')\n",
    "b = Value(3.0, label='b')\n",
    "d = a * b    ; d.label = 'd'\n",
    "e = a + b    ; e.label = 'e'\n",
    "f = d * e    ; f.label = 'f'\n",
    "\n",
    "f.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Gradient:\n",
      "    f: 1.000\n",
      "    d: 1.000\n",
      "    e: -6.000\n",
      "    a: -3.000\n",
      "    b: -8.000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"\"\"\n",
    "Gradient:\n",
    "    f: {f.grad:.3f}\n",
    "    d: {d.grad:.3f}\n",
    "    e: {e.grad:.3f}\n",
    "    a: {a.grad:.3f}\n",
    "    b: {b.grad:.3f}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7 (tags/v3.10.7:6cc6b13, Sep  5 2022, 14:08:36) [MSC v.1933 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "93d343a67186057eba7cc3f4fb38dbcf94a41bc6d951d30420561a381ab22bcf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
